{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.losses import BinaryFocalCrossentropy\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "IMG_SIZE = (380, 380)  # EfficientNetB4 requires 380x380\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS_PHASE1 = 15\n",
    "EPOCHS_PHASE2 = 10\n",
    "CLASS_NAMES = ['DR', 'MH', 'ODC', 'TSLN', 'DN', 'MYA', 'ARMD', 'BRVO', 'ODP', 'ODE', 'LS', 'RS', 'CSR', 'CRS']\n",
    "\n",
    "# Paths\n",
    "train_labels_path = \"retinal-disease-classification/Training_Set/RFMiD_Training_Labels.csv\"\n",
    "val_labels_path = \"retinal-disease-classification/Evaluation_Set/RFMiD_Validation_Labels.csv\"\n",
    "test_labels_path = \"retinal-disease-classification/Test_Set/RFMiD_Testing_Labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "def load_and_preprocess_data():\n",
    "    train_labels = pd.read_csv(train_labels_path)\n",
    "    val_labels = pd.read_csv(val_labels_path)\n",
    "    test_labels = pd.read_csv(test_labels_path)\n",
    "\n",
    "    selected_diseases = CLASS_NAMES\n",
    "    \n",
    "    def process_df(df):\n",
    "        selected_diseases = list(set(CLASS_NAMES) & set(df.columns))\n",
    "        df = df[['ID', 'Disease_Risk'] + selected_diseases].copy()\n",
    "        df['Disease_Risk'] = (df[selected_diseases].sum(axis=1) > 0).astype(int)\n",
    "        return df\n",
    "\n",
    "    return (\n",
    "        process_df(train_labels),\n",
    "        process_df(val_labels),\n",
    "        process_df(test_labels)\n",
    "    )\n",
    "\n",
    "train_df, val_df, test_df = load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Data Generator with Mixup\n",
    "class AdvancedDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, img_dir, df, batch_size=32, img_size=IMG_SIZE, \n",
    "                 augment=False, shuffle=True, mixup_alpha=0.4, **kwargs):\n",
    "        super().__init__(**kwargs) \n",
    "         \n",
    "        self.img_dir = img_dir\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.augment = augment\n",
    "        self.shuffle = shuffle\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        self.indices = np.arange(len(df))\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        # Augmentation configurations\n",
    "        self.augmenter = ImageDataGenerator(\n",
    "            rotation_range=25,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True,\n",
    "            brightness_range=[0.8, 1.2]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index*self.batch_size : (index+1)*self.batch_size]\n",
    "        batch_df = self.df.iloc[batch_indices]\n",
    "        \n",
    "        # Load images and labels\n",
    "        X, y = self._load_data(batch_df)\n",
    "        \n",
    "        # Apply Mixup\n",
    "        if self.augment and self.mixup_alpha > 0:\n",
    "            X, y = self._apply_mixup(X, y)\n",
    "            \n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def _load_data(self, batch_df):\n",
    "        X = np.empty((len(batch_df), *self.img_size, 3))\n",
    "        y = np.empty((len(batch_df), len(CLASS_NAMES)))   # Disease_Risk + diseases\n",
    "        \n",
    "        for i, (_, row) in enumerate(batch_df.iterrows()):\n",
    "            img_path = os.path.join(self.img_dir, f\"{row['ID']}.png\")\n",
    "            img = load_img(img_path, target_size=self.img_size)\n",
    "            img_array = img_to_array(img)\n",
    "            \n",
    "            # Apply augmentation only to diseased samples\n",
    "            if self.augment and row['Disease_Risk'] == 1:\n",
    "                img_array = self.augmenter.random_transform(img_array)\n",
    "                \n",
    "            X[i] = preprocess_input(img_array)  # EfficientNet preprocessing\n",
    "            y[i] = row[CLASS_NAMES].values\n",
    "            \n",
    "        return X, y.astype(np.float32)\n",
    "\n",
    "    def _apply_mixup(self, X, y):\n",
    "        lam = np.clip(np.random.beta(self.mixup_alpha, self.mixup_alpha), 0.2, 0.8)\n",
    "        rand_index = np.random.permutation(len(X))\n",
    "        \n",
    "        mixed_X = lam * X + (1 - lam) * X[rand_index]\n",
    "        mixed_y = lam * y + (1 - lam) * y[rand_index]\n",
    "        return mixed_X, mixed_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators\n",
    "train_gen = AdvancedDataGenerator(\n",
    "    \"retinal-disease-classification/Training_Set/Training\",\n",
    "    train_df,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    augment=True,\n",
    "    mixup_alpha=0.4\n",
    ")\n",
    "\n",
    "val_gen = AdvancedDataGenerator(\n",
    "    \"retinal-disease-classification/Evaluation_Set/Validation\",\n",
    "    val_df,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "test_gen = AdvancedDataGenerator(\n",
    "    \"retinal-disease-classification/Test_Set/Test\",\n",
    "    test_df,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch, y_batch = train_gen[0]\n",
    "print(\"Shape of X_batch:\", X_batch.shape)  # Should be (batch_size, 380, 380, 3)\n",
    "print(\"Shape of y_batch:\", y_batch.shape)  # Should be (batch_size, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights\n",
    "def calculate_class_weights(df):\n",
    "    weights = {}\n",
    "    for idx, disease in enumerate(['Disease_Risk'] + CLASS_NAMES):\n",
    "        cls_weights = compute_class_weight(\n",
    "            'balanced',\n",
    "            classes=np.array([0, 1]),\n",
    "            y=df[disease]\n",
    "        )\n",
    "        weights[idx] = {0: cls_weights[0], 1: cls_weights[1]}\n",
    "    return weights\n",
    "\n",
    "class_weights = calculate_class_weights(train_df)\n",
    "class_weights = {idx: weights[1] for idx, weights in class_weights.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "def build_model():\n",
    "    base_model = EfficientNetB4(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(*IMG_SIZE, 3)\n",
    "    )\n",
    "    base_model.trainable = False  # Freeze initially\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(512, activation='swish', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='swish'),\n",
    "        Dense(len(CLASS_NAMES), activation='sigmoid')  # Disease_Risk + diseases\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def focal_loss(alpha=0.25, gamma=2.0):\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        # Cast to float32\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        \n",
    "        # Compute element-wise binary crossentropy using Keras backend.\n",
    "        ce = tf.keras.backend.binary_crossentropy(y_true, y_pred)  \n",
    "        # ce now has shape (batch_size, num_classes)\n",
    "        \n",
    "        # Compute p_t element-wise\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        \n",
    "        # Compute alpha factor element-wise\n",
    "        alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        \n",
    "        # Compute modulating factor element-wise\n",
    "        modulating_factor = tf.pow(1.0 - p_t, gamma)\n",
    "        \n",
    "        # Final loss element-wise\n",
    "        loss = alpha_factor * modulating_factor * ce\n",
    "        return loss  # Optionally, you can reduce_mean over the batch or last axis.\n",
    "    \n",
    "    return loss_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_y_true = tf.convert_to_tensor(y_batch[:5], dtype=tf.float32)  # (5, 14)\n",
    "sample_y_pred = tf.random.uniform(sample_y_true.shape, 0, 1)           # (5, 14)\n",
    "loss_fn = focal_loss(alpha=0.25, gamma=2.0)\n",
    "print(loss_fn(sample_y_true, sample_y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"y_true shape:\", sample_y_true.shape)\n",
    "print(\"y_pred shape:\", sample_y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Train head\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=focal_loss(gamma=2.0, alpha=0.25),\n",
    "    metrics=[AUC(name='auc', multi_label=True)]\n",
    ")\n",
    "\n",
    "phase1_callbacks = [\n",
    "    EarlyStopping(patience=3, monitor='val_auc', mode='max', verbose=1),\n",
    "    ModelCheckpoint('phase1_best.keras', save_best_only=True),\n",
    "    ReduceLROnPlateau(factor=0.5, patience=2)\n",
    "]\n",
    "\n",
    "history_phase1 = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=EPOCHS_PHASE1,\n",
    "    callbacks=phase1_callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Fine-tune\n",
    "model = tf.keras.models.load_model('phase1_best.keras', custom_objects={'loss_fn': focal_loss()}) # Load best weights from phase 1\n",
    "model.layers[0].trainable = True  # Unfreeze base model\n",
    "\n",
    "# Set last 150 layers trainable\n",
    "for layer in model.layers[0].layers[-150:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss=focal_loss(gamma=2.0, alpha=0.25),\n",
    "    metrics=[AUC(name='auc', multi_label=True)]\n",
    ")\n",
    "\n",
    "phase2_callbacks = [\n",
    "    EarlyStopping(patience=2, monitor='val_auc', mode='max', verbose=1),\n",
    "    ModelCheckpoint('final_model.keras', save_best_only=True),\n",
    "    ReduceLROnPlateau(factor=0.2, patience=1)\n",
    "]\n",
    "\n",
    "history_phase2 = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=EPOCHS_PHASE2,\n",
    "    callbacks=phase2_callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation with Test-Time Augmentation\n",
    "def evaluate_with_tta(model, generator, n_tta=5):\n",
    "    y_true, y_pred = [], []\n",
    "    # tta_preds = []\n",
    "    \n",
    "    for i in range(len(generator)):\n",
    "        X, y = generator[i]\n",
    "        batch_preds = np.zeros_like(y)\n",
    "        \n",
    "        for _ in range(n_tta):\n",
    "            # Create augmented versions\n",
    "            aug_X = np.array([generator.augmenter.random_transform(img) for img in X])\n",
    "            batch_preds += model.predict(aug_X)\n",
    "            \n",
    "        # Average predictions\n",
    "        batch_preds /= n_tta  # Average predictions\n",
    "        y_true.append(y)\n",
    "        y_pred.append(batch_preds)\n",
    "        # avg_pred = np.mean(batch_preds, axis=0)\n",
    "        # tta_preds.append(avg_pred)\n",
    "        # y_true.append(y)\n",
    "        \n",
    "    return np.vstack(y_true), np.vstack(y_pred)\n",
    "\n",
    "model = tf.keras.models.load_model('final_model.keras', custom_objects={'loss_fn': focal_loss()})\n",
    "y_true, y_pred = evaluate_with_tta(model, test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate reports\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(\n",
    "    y_true[:, 1:],  # Skip Disease_Risk\n",
    "    (y_pred[:, 1:] > 0.5).astype(int),\n",
    "    target_names=CLASS_NAMES\n",
    "))\n",
    "\n",
    "print(\"\\nConfusion Matrices:\")\n",
    "for idx, disease in enumerate(CLASS_NAMES):\n",
    "    cm = confusion_matrix(y_true[:, idx+1], (y_pred[:, idx+1] > 0.5).astype(int))\n",
    "    plt.figure()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f\"{disease} Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_phase1.history['auc'], label='Phase1 Train')\n",
    "plt.plot(history_phase1.history['val_auc'], label='Phase1 Val')\n",
    "plt.plot(np.arange(EPOCHS_PHASE1, EPOCHS_PHASE1+len(history_phase2.history['auc'])), \n",
    "         history_phase2.history['auc'], label='Phase2 Train')\n",
    "plt.plot(np.arange(EPOCHS_PHASE1, EPOCHS_PHASE1+len(history_phase2.history['val_auc'])), \n",
    "         history_phase2.history['val_auc'], label='Phase2 Val')\n",
    "plt.title('AUC History')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_phase1.history['loss'], label='Phase1 Train')\n",
    "plt.plot(history_phase1.history['val_loss'], label='Phase1 Val')\n",
    "plt.plot(np.arange(EPOCHS_PHASE1, EPOCHS_PHASE1+len(history_phase2.history['loss'])), \n",
    "         history_phase2.history['loss'], label='Phase2 Train')\n",
    "plt.plot(np.arange(EPOCHS_PHASE1, EPOCHS_PHASE1+len(history_phase2.history['val_loss'])), \n",
    "         history_phase2.history['val_loss'], label='Phase2 Val')\n",
    "plt.title('Loss History')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
