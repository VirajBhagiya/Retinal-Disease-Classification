{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.losses import BinaryFocalCrossentropy\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "IMG_SIZE = (380, 380)  # EfficientNetB4 requires 380x380\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS_PHASE1 = 15\n",
    "EPOCHS_PHASE2 = 10\n",
    "CLASS_NAMES = ['DR', 'MH', 'ODC', 'TSLN', 'DN', 'MYA', 'ARMD', 'BRVO', 'ODP', 'ODE', 'LS', 'RS', 'CSR', 'CRS']\n",
    "\n",
    "# Paths\n",
    "train_labels_path = \"retinal-disease-classification/Training_Set/RFMiD_Training_Labels.csv\"\n",
    "val_labels_path = \"retinal-disease-classification/Evaluation_Set/RFMiD_Validation_Labels.csv\"\n",
    "test_labels_path = \"retinal-disease-classification/Test_Set/RFMiD_Testing_Labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "def load_and_preprocess_data():\n",
    "    train_labels = pd.read_csv(train_labels_path)\n",
    "    val_labels = pd.read_csv(val_labels_path)\n",
    "    test_labels = pd.read_csv(test_labels_path)\n",
    "\n",
    "    selected_diseases = CLASS_NAMES\n",
    "    \n",
    "    def process_df(df):\n",
    "        df = df[['ID', 'Disease_Risk'] + selected_diseases].copy()\n",
    "        df['Disease_Risk'] = (df[selected_diseases].sum(axis=1) > 0).astype(int)\n",
    "        return df\n",
    "\n",
    "    return (\n",
    "        process_df(train_labels),\n",
    "        process_df(val_labels),\n",
    "        process_df(test_labels)\n",
    "    )\n",
    "\n",
    "train_df, val_df, test_df = load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Data Generator with Mixup\n",
    "class AdvancedDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, img_dir, df, batch_size=32, img_size=IMG_SIZE, \n",
    "                 augment=False, shuffle=True, mixup_alpha=0.4):\n",
    "        self.img_dir = img_dir\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.augment = augment\n",
    "        self.shuffle = shuffle\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        self.indices = np.arange(len(df))\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        # Augmentation configurations\n",
    "        self.augmenter = ImageDataGenerator(\n",
    "            rotation_range=25,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True,\n",
    "            brightness_range=[0.8, 1.2]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index*self.batch_size : (index+1)*self.batch_size]\n",
    "        batch_df = self.df.iloc[batch_indices]\n",
    "        \n",
    "        # Load images and labels\n",
    "        X, y = self._load_data(batch_df)\n",
    "        \n",
    "        # Apply Mixup\n",
    "        if self.augment and self.mixup_alpha > 0:\n",
    "            X, y = self._apply_mixup(X, y)\n",
    "            \n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def _load_data(self, batch_df):\n",
    "        X = np.empty((len(batch_df), *self.img_size, 3))\n",
    "        y = np.empty((len(batch_df), len(CLASS_NAMES) + 1))  # Disease_Risk + diseases\n",
    "        \n",
    "        for i, (_, row) in enumerate(batch_df.iterrows()):\n",
    "            img_path = os.path.join(self.img_dir, f\"{row['ID']}.png\")\n",
    "            img = load_img(img_path, target_size=self.img_size)\n",
    "            img_array = img_to_array(img)\n",
    "            \n",
    "            # Apply augmentation only to diseased samples\n",
    "            if self.augment and row['Disease_Risk'] == 1:\n",
    "                img_array = self.augmenter.random_transform(img_array)\n",
    "                \n",
    "            X[i] = preprocess_input(img_array)  # EfficientNet preprocessing\n",
    "            y[i] = row[['Disease_Risk'] + CLASS_NAMES].values\n",
    "            \n",
    "        return X, y\n",
    "\n",
    "    def _apply_mixup(self, X, y):\n",
    "        lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n",
    "        rand_index = np.random.permutation(len(X))\n",
    "        \n",
    "        mixed_X = lam * X + (1 - lam) * X[rand_index]\n",
    "        mixed_y = lam * y + (1 - lam) * y[rand_index]\n",
    "        return mixed_X, mixed_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators\n",
    "train_gen = AdvancedDataGenerator(\n",
    "    \"retinal-disease-classification/Training_Set/Training\",\n",
    "    train_df,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    augment=True,\n",
    "    mixup_alpha=0.4\n",
    ")\n",
    "\n",
    "val_gen = AdvancedDataGenerator(\n",
    "    \"retinal-disease-classification/Evaluation_Set/Validation\",\n",
    "    val_df,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "test_gen = AdvancedDataGenerator(\n",
    "    \"retinal-disease-classification/Test_Set/Test\",\n",
    "    test_df,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights\n",
    "def calculate_class_weights(df):\n",
    "    weights = {}\n",
    "    for idx, disease in enumerate(['Disease_Risk'] + CLASS_NAMES):\n",
    "        cls_weights = compute_class_weight(\n",
    "            'balanced',\n",
    "            classes=np.array([0, 1]),\n",
    "            y=df[disease]\n",
    "        )\n",
    "        weights[idx] = {0: cls_weights[0], 1: cls_weights[1]}\n",
    "    return weights\n",
    "\n",
    "class_weights = calculate_class_weights(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "def build_model():\n",
    "    base_model = EfficientNetB4(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(*IMG_SIZE, 3)\n",
    "    )\n",
    "    base_model.trainable = False  # Freeze initially\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(512, activation='swish', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='swish'),\n",
    "        Dense(len(CLASS_NAMES) + 1, activation='sigmoid')  # Disease_Risk + diseases\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Focal Loss\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        bce = tf.keras.losses.BinaryCrossentropy(reduction='none')\n",
    "        ce = bce(y_true, y_pred)\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        modulating_factor = (1.0 - p_t) ** gamma\n",
    "        return tf.reduce_mean(alpha_factor * modulating_factor * ce, axis=-1)\n",
    "    return loss_fn\n",
    "\n",
    "# Two-Phase Training\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Train head\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=focal_loss(gamma=2.0, alpha=0.25),\n",
    "    metrics=[AUC(name='auc', multi_label=True)]\n",
    ")\n",
    "\n",
    "phase1_callbacks = [\n",
    "    EarlyStopping(patience=3, monitor='val_auc', mode='max', verbose=1),\n",
    "    ModelCheckpoint('phase1_best.keras', save_best_only=True),\n",
    "    ReduceLROnPlateau(factor=0.5, patience=2)\n",
    "]\n",
    "\n",
    "history_phase1 = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=EPOCHS_PHASE1,\n",
    "    callbacks=phase1_callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Fine-tune\n",
    "model.load_weights('phase1_best.keras')  # Load best weights from phase 1\n",
    "model.layers[0].trainable = True  # Unfreeze base model\n",
    "\n",
    "# Set last 150 layers trainable\n",
    "for layer in model.layers[0].layers[-150:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss=focal_loss(gamma=2.0, alpha=0.25),\n",
    "    metrics=[AUC(name='auc', multi_label=True)]\n",
    ")\n",
    "\n",
    "phase2_callbacks = [\n",
    "    EarlyStopping(patience=2, monitor='val_auc', mode='max', verbose=1),\n",
    "    ModelCheckpoint('final_model.keras', save_best_only=True),\n",
    "    ReduceLROnPlateau(factor=0.2, patience=1)\n",
    "]\n",
    "\n",
    "history_phase2 = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=EPOCHS_PHASE2,\n",
    "    callbacks=phase2_callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation with Test-Time Augmentation\n",
    "def evaluate_with_tta(model, generator, n_tta=5):\n",
    "    y_true = []\n",
    "    tta_preds = []\n",
    "    \n",
    "    for i in range(len(generator)):\n",
    "        X, y = generator[i]\n",
    "        batch_preds = []\n",
    "        \n",
    "        for _ in range(n_tta):\n",
    "            # Create augmented versions\n",
    "            aug_X = generator.augmenter.random_transform(X)\n",
    "            batch_preds.append(model.predict(aug_X))\n",
    "            \n",
    "        # Average predictions\n",
    "        avg_pred = np.mean(batch_preds, axis=0)\n",
    "        tta_preds.append(avg_pred)\n",
    "        y_true.append(y)\n",
    "        \n",
    "    return np.vstack(y_true), np.vstack(tta_preds)\n",
    "\n",
    "y_true, y_pred = evaluate_with_tta(model.load_weights('final_model.keras'), test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate reports\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(\n",
    "    y_true[:, 1:],  # Skip Disease_Risk\n",
    "    (y_pred[:, 1:] > 0.5).astype(int),\n",
    "    target_names=CLASS_NAMES\n",
    "))\n",
    "\n",
    "print(\"\\nConfusion Matrices:\")\n",
    "for idx, disease in enumerate(CLASS_NAMES):\n",
    "    cm = confusion_matrix(y_true[:, idx+1], (y_pred[:, idx+1] > 0.5).astype(int))\n",
    "    plt.figure()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f\"{disease} Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_phase1.history['auc'], label='Phase1 Train')\n",
    "plt.plot(history_phase1.history['val_auc'], label='Phase1 Val')\n",
    "plt.plot(np.arange(EPOCHS_PHASE1, EPOCHS_PHASE1+len(history_phase2.history['auc'])), \n",
    "         history_phase2.history['auc'], label='Phase2 Train')\n",
    "plt.plot(np.arange(EPOCHS_PHASE1, EPOCHS_PHASE1+len(history_phase2.history['val_auc'])), \n",
    "         history_phase2.history['val_auc'], label='Phase2 Val')\n",
    "plt.title('AUC History')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_phase1.history['loss'], label='Phase1 Train')\n",
    "plt.plot(history_phase1.history['val_loss'], label='Phase1 Val')\n",
    "plt.plot(np.arange(EPOCHS_PHASE1, EPOCHS_PHASE1+len(history_phase2.history['loss'])), \n",
    "         history_phase2.history['loss'], label='Phase2 Train')\n",
    "plt.plot(np.arange(EPOCHS_PHASE1, EPOCHS_PHASE1+len(history_phase2.history['val_loss'])), \n",
    "         history_phase2.history['val_loss'], label='Phase2 Val')\n",
    "plt.title('Loss History')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
